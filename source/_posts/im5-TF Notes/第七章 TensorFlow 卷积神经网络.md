---
title: 第七章 TensorFlow 卷积神经网络
categories:      
    Machine Learning      
tags: [Deep Learning,TensorFlow]
date: 2019-6-6 22:55:03
---

# 摘要

本节主要是学习TensorFlow的相关学习笔记，主要是基础的学习路线，包括简单的实例笔记等。

内容包括如下：

- TensorFlow 卷积神经网络基础
- 卷积神经网络常用的函数
- CNN原理
- 代码实现

> 提示本部分是一个PDF手稿，暂时未整理排版，只能在电脑端预览本部分的PDF笔记,手机上的PDF笔记将不会显示出来。

- [x] Edit By Porter, 积水成渊,蛟龙生焉。

<!-- more -->

# 第七章 TensorFlow 卷积神经网络

## 7.1 基本概念

- 全连接NN

每个神经元与前后相邻层的每一个神经元都有连接关系，输入时特征，输出为预测结果。

-  卷积(~~Convolutional~~)

卷积是一种有效提取图像特征的方法，一般用一个正方形的卷积核，遍历图像上的每一像素点。图片上与卷积核对应重叠区域的像素值与卷积核上对应点的权值相乘再求和，再加上偏置，得到一个输出像素。

- 全零填充(padding)

全零填充可以保证，卷积后的图像尺寸可以保持输入输出大小一致。


## 7.2 神经网络的常用函数

### 7.2.1 tf.nn.conv2d(输入图像的描述， 卷积核描述，核滑动步长描述，padding='VALID')

#### 输入图像的描述

给出一次喂入的图片数量；像素大小；颜色通道数量

例如: [5, 28, ,28, 3] ,表示一次喂入5张图片，每张图像的像素的大小28x28，颜色通道数为3个。

#### 卷积核描述

例如 [3, 3, 1, 16] 表示三行三列1个通道的核函数共16个这样的核函数

一共这样的核函数，也就是说卷积操作后输出图像的深度是16，也就是输出为16通道

#### 核滑动步长

例如： [1, 1, 1, 1] 第一个和最后一个1都是固定的，中间两个1是表示为横向坐标和纵向坐标。

#### padding 

padding = 'VALID' 表示不使用填充

padding = “SAME" 表示全零填充


### 7.2.2 tf.nn.avg_pool(输入描述，池化描述，池化核滑动步长的描述，padding) 

函数功能：平均池化函数

输入描述：给出一次输入batch张图片的描述，例如：[batch, 28, 28, 6]

表示图像像素大小为28x28 通道数为6。

池化核滑动步长： 例如：[1, 2, 2, 1] 第一个和最后一个都固定为1, 中间两个表示，横纵坐标像素个数。


### 7.2.3 tf.nn.max_pool(输入描述，池化描述，池化核滑动步长的描述，padding) 

函数功能：最大池化函数

输入描述：给出一次输入batch张图片的描述，例如：[batch, 28, 28, 6]

表示图像像素大小为28x28 通道数为6。

池化核滑动步长： 例如：[1, 2, 2, 1] 第一个和最后一个都固定为1, 中间两个表示，横纵坐标像素个数。


### 7.2.4  tf.nn.dropout(上层输入，暂时舍弃神经元的概率)

在神经网络的计算过程中，为了减少过多参数常使用dropout的方法，将一部分神经元按照一定概率从神经网络中舍弃。这种舍弃是临时的，仅在训练师舍弃一些神经元。

在使用神经网络时，会把所有的神经元恢复到神经网络中。

- dropout 可以有效的减少过拟合

- 常常在前向传播构建神经网络中使用dropout来减小过拟合加快模型的训练速度

- dropout 一般放到全连接网络中

#### 输出 = tf.nn.dropout(上层输入，暂时舍弃神经元的概率)

这样就有指定概率的神经元随机被置零，置零的神经元不参加当前轮的参数优化


## 7.3 CNN： 借助卷积核(kernel)提取特征后，送入全连接网络

卷积神经网络可以认为由两部分组成，一部分是对输入图片进行特征提取，另一部分是全连接网络。只不过喂入全连接网络的不再是原始图片，而是经过若干次卷积，激活和池化后的特征信息。

目前经典的卷积神经网络有很多经典的结构，比如：Lenet-5、Alenet、VGGNet、GoogleNet、ResNet.每一种都是以卷积、激活、池化四种操作为基础展开。


### 7.3.1 卷积神经网络的连个概念

局部感知野、权值共享

#### 局部感知野

卷积神经网络的出现是受到了生物处理过程的启发，因为神经元之间的连接模式类似于动物的视觉皮层组织。

![人脑的视觉结构](https://s2.ax1x.com/2019/06/22/ZplRhR.png)

个体皮层神经元仅在被称为感受野的视野受限区域中对刺激作出反应，不同神经元的**感受野**部分重叠，使得它们能够覆盖整个视野。

#### 共享权重和偏置

理解卷积的一个简单方法是考虑作用于矩阵的滑动窗函数。
在下面的例子中，给定输入矩阵 I 和核 K，得到卷积输出。将 3×3 核 K（有时称为滤波器或特征检测器）与输入矩阵逐元素地相乘以得到输出卷积矩阵中的一个元素。
所有其他元素都是通过在 I 上滑动窗口获得的：

![共享权值的核卷积](https://s2.ax1x.com/2019/06/22/ZpcvnO.png)

> 简单理解就是一张图片的一个通道中的像素是与同样的核函数进行卷积计算的，可以理解为共享相同的权值和偏置。


### 7.3.2 卷积层

![CNN](https://s2.ax1x.com/2019/06/22/ZpKafx.png)

卷积神经网络如上图所示，经过卷积层和后面的全连接网络，其后面的全连接和之前的全连接形式相当，是人工神经网络的样子。但是输入层却有所不同了，之前的人工神经网络的输入层是现将图片打散成一个一维的数据矩阵，在CNN中，数据的输入可以直接是RGB通道的图片。

-  CNN的输入如果是n个深度的图像数据，那么卷积核(滤波器)的深度也应该是要n个深度。

如下图，三个通道的图像数据矩阵

![RGB三通道的数据图像矩阵](https://s2.ax1x.com/2019/06/22/ZpK2tI.png)

卷基层将会也是3个深度的核矩阵（滤波器）,卷积的计算过程如下：

- 矩阵的区域对应卷积核(滤波器)的重叠区域进行权值相乘并最终求和得到一个数值结果，并将该结果作为输出结果。
- 滤波器将滑过整个图像，重复相同的点积运算。这里

- 滤波器必须具有与输入图像相同数量的通道；
- 网络越深，使用的滤波器就越多；拥有的滤波器越多，获得的边缘和特征检测就越多；

![卷积运算](https://s2.ax1x.com/2019/06/22/ZpKTBQ.gif)

### 7.3.3 卷积层的输出尺寸

输出宽度： $\frac{W-F_{w}+2P}{S}+1$

输出高度：$\frac{H-F_{h}+2P}{S}+1$

其中：

- $W$ ：输入图像的宽度
- $H$ ：输入图像的高度
- $F_{w}$  ：滤波器或内核的宽度
- $F_{h}$ ：滤波器的高度
- P ：填充
- S ：移动步幅

       卷积层输出的通道数等于卷积操作期间使用的滤波器的个数。

## 7.4 卷积神经网络的代码实现

卷积、激活函数、池化三个过程可以说是卷基层的标配（有时也会去掉池化）。卷积计算后的激活函数大多使用的ReLU作为激活函数，这样可以保证卷积后的数值如果小于0 则用零替换，否则保持正数的数值不变。


```python
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variables(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, W):
    return tf.nn.conv2d(x, W, stides=[1, 1, 1, 1], padding='SAME')

def max_pol_2x2(x):
    return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')


# 第一层卷积

W_conv1 = weight_variable([5,5,1,32])
b_conv1 = bias_variable([32])
h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1) # 卷积和激活
h_pool1 = max_pool_2x2(h_conv1) # 池化

# 第二层卷积
W_conv2 = weight_variable([5,5,32,64])
b_conv2 = bias_variable([64])
h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2) # 卷积和激活
h_pool2 = max_pool_2x2(h_conv2) # 池化

# 之后是全连接

w_fc1 = weight_variable([7*7*64, 1024])
b_fc1 = bias_variabl([1024])
h_pool2_flat = tf.reshape(h_hool2, [-1. 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2, [-1, 7*7*64])+b_fc1)

# 使用dropout, keep_prob是一个占位符,训练时为0.5， 测试时为1
keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

# 再使用全连接将一层前向传播变成onehot编码模式的输出
W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])
y_conv = tf.matmul(h_fc1_drop, W_fc2)+b_fc2


                   
```


